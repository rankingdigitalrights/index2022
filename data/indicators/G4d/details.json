{"id":"G04d","name":"G4d","category":"governance","label":"Impact assessment: Algorithmic systems","description":"The company should conduct regular, comprehensive, and credible due diligence, such as through robust <a href='#glossary-hria'>human rights impact assessments</a>, to identify how all aspects of its policies and practices related to the development and use of <a href='#glossary-algsys'>algorithmic systems</a> affect users' fundamental rights to freedom of expression and information, to privacy, and to <a href='#glossary-discrimination'>non-discrimination</a>, and to mitigate any risks posed by those impacts.","guidance":"<p>There are a variety of ways in which algorithmic systems may pose harms to human rights. The development of such systems can rely on user information, often without the knowledge or explicit, informed consent of the data subject, constituting a privacy violation. Such systems can also cause or contribute to expression and information harms. In addition, the purpose of many algorithmic decision-making systems is to automate the personalization of users' experiences on the basis of collected and inferred user information, which may cause or contribute to discrimination. Companies should therefore conduct human rights risk assessments related to their development and use of algorithms, as recommended by the Council of Europe in its <a href='https://search.coe.int/cm/pages/result_details.aspx?objectid=09000016809e1154'>Recommendation on the human rights impacts of algorithmic systems </a>(2020).</p> <p>This indicator examines whether companies conduct robust, regular, and accountable human rights risk assessment assessments that evaluate their policies and practices relating to their development and deployment of algorithmic systems. These assessments should be part of the company's formal, systematic due diligence activities that are aimed at ensuring that a company's decisions and practices do not cause, contribute to, or exacerbate human rights harms.</p> <p>Assessments enable companies to identify possible risks of their development and deployment of algorithmic systems on users' human rights and to take steps to mitigate possible harms if they are identified.</p> <p>Note that this indicator does not expect companies to publish detailed results of their human rights impact assessments, since assessments may include sensitive information. Rather, it expects that companies should disclose that they conduct HRIAs and provide information on what their HRIA process encompasses.</p>","isParent":false,"hasParent":true}