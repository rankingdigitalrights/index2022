{"id":"F01d","name":"F1d","category":"freedom","label":"Access to algorithmic system use policies","description":"The company should offer policies related to their use of <a href='#glossary-algorithm'>algorithms</a> that are <a href='#glossary-easytofind'>easy for users to find</a> and <a href='#glossary-easytounderstand'>understand</a>.","guidance":"<p>The use of algorithmic systems can have adverse effects on fundamental human rightsâ€”and specifically, on the right to freedom of expression and information as well as the right to non-discrimination. In addition to clearly committing to respect and protect human rights as they develop and deploy these technologies (see Indicator G1, Element 3), companies should also publish policies that clearly describe the terms for how they use algorithmic systems across their service and platforms. Similar to having terms of service policies or user agreements that outline the terms for what types of content or activities are prohibited, companies that use algorithmic systems with the potential to cause human rights harms should publish a clear and accessible policy stating the nature and functions of these systems. As recommended by the Council of Europe's <a href='https://search.coe.int/cm/pages/result_details.aspx?objectid=09000016809e1154'>Recommendation on the human rights impacts of algorithmic systems </a>(2020), this policy should be easy to find, presented in plain language, and contains options for users to manage settings.</p> <p>Note that in this indicator, we are looking for a policy that explains terms for how the company deploys algorithmic systems across its platforms and services. We also look for companies to disclose terms that outline how they develop and test algorithmic systems, which is addressed in Indicator P1b.</p>","isParent":false,"hasParent":true}